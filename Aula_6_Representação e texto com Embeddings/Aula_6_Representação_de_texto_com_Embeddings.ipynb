{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuigiFalcade13/fatec_PLN_Codes/blob/main/Aula_6_Representa%C3%A7%C3%A3o_de_texto_com_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Luigi Falcade de Souza**"
      ],
      "metadata": {
        "id": "FnJsKDMkYBl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aula 06 - Representação de texto com Embeddings\n",
        "\n",
        "## Exemplo 01 - Word2Vec"
      ],
      "metadata": {
        "id": "a_DiDmRdToB6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjwpiHKn3K9C",
        "outputId": "91992fef-f031-4bfc-c2f8-49e5790694a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridade entre 'cachorro' e 'gato': 0.13149002\n"
          ]
        }
      ],
      "source": [
        "# importação da ferramenta de vetorização\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# texto a ser analisado\n",
        "corpus = [\n",
        "    [\"o\", \"cachorro\", \"está\", \"latindo\"],\n",
        "    [\"o\", \"gato\", \"está\", \"miando\"]\n",
        "]\n",
        "\n",
        "# realizar o treinamento das palavras\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
        "  # sentences -- indicar o texto a ser analisado.\n",
        "  # window -- faixa de palavras (antes e depois) que serão analisadas\n",
        "  # vector_size -- Dimensão do vetor\n",
        "  # min_count=1\n",
        "  #sg - representa o modelo a ser utiizado\n",
        "    # 0 - CBOW: busca a palavra a partir do contexto\n",
        "    # 1 - Skip-gram: busco o contexto a partir da palavr\n",
        "\n",
        "# vetorização da palavra desejada\n",
        "vector = model.wv['cachorro']\n",
        "\n",
        "# comparação de vetores.\n",
        "similarity = model.wv.similarity('cachorro', 'gato')\n",
        "\n",
        "# impressão do resultado\n",
        "print(\"Similaridade entre 'cachorro' e 'gato':\", similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exemplo 2**"
      ],
      "metadata": {
        "id": "VJY8jN2oZAzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Corpus de treinamento com frases simples\n",
        "corpus = [\n",
        "    [\"o\", \"cachorro\", \"está\", \"latindo\", \"no\", \"quintal\"],  # Exemplo de frase com cachorro\n",
        "    [\"o\", \"gato\", \"está\", \"miando\", \"no\", \"telhado\"],  # Exemplo com gato\n",
        "    [\"o\", \"pássaro\", \"está\", \"voando\", \"no\", \"céu\"],  # Exemplo com pássaro\n",
        "    [\"a\", \"bola\", \"está\", \"rolando\", \"no\", \"chão\"],  # Exemplo com bola\n",
        "    [\"a\", \"criança\", \"está\", \"brincando\", \"com\", \"o\", \"cachorro\"],  # Frase com criança e cachorro\n",
        "    [\"o\", \"gato\", \"e\", \"o\", \"rato\", \"são\", \"inimigos\"],  # Frase com gato e rato\n",
        "    [\"a\", \"água\", \"está\", \"quente\", \"na\", \"caneca\"],  # Exemplo com água\n",
        "    [\"o\", \"sol\", \"está\", \"brilhando\", \"no\", \"céu\"],  # Exemplo com sol\n",
        "    [\"a\", \"lua\", \"está\", \"cheia\", \"hoje\"],  # Exemplo com lua\n",
        "    [\"a\", \"computador\", \"está\", \"ligado\", \"na\", \"mesa\"]  # Exemplo com computador\n",
        "]\n",
        "\n",
        "# Treinando o modelo Word2Vec\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# 'vector_size=100' define o tamanho do vetor que representa cada palavra.\n",
        "# 'window=5' define o número de palavras ao redor da palavra central que serão usadas como contexto.\n",
        "# 'min_count=1' diz que consideraremos todas as palavras que aparecem pelo menos uma vez.\n",
        "# 'sg=1' usa o modelo Skip-gram (em vez do CBOW que seria com sg=0).\n",
        "\n",
        "# Calculando a similaridade entre \"cachorro\" e \"gato\"\n",
        "print(f\"Similidaridade entre cachorro e gato: {model.wv.similarity('cachorro', 'gato')}\")\n",
        "# \"cachorro\" e \"gato\" são animais frequentemente mencionados em contextos semelhantes (animais domésticos), então esperamos uma alta similaridade.\n",
        "\n",
        "# Calculando a similaridade entre \"cachorro\" e \"bola\"\n",
        "print(f\"Similidaridade entre cachorro e bola: {model.wv.similarity('cachorro', 'bola')}\")\n",
        "# \"cachorro\" e \"bola\" são relacionados em contextos de brincadeiras, mas a similaridade será menor do que entre \"cachorro\" e \"gato\".\n",
        "\n",
        "# Calculando a similaridade entre \"céu\" e \"lua\"\n",
        "print(f\"Similidaridade entre céu e lua: {model.wv.similarity('céu', 'lua')}\")\n",
        "# \"céu\" e \"lua\" estão relacionados em contextos naturais (ambos associados ao ambiente exterior), então esperamos uma boa similaridade.\n",
        "\n",
        "# Calculando a similaridade entre \"computador\" e \"mesa\"\n",
        "print(f\"Similidaridade entre computador e mesa: {model.wv.similarity('computador', 'mesa')}\")\n",
        "# \"computador\" e \"mesa\" estão associados frequentemente em contextos de escritório ou estudo, mas a similaridade pode ser mais moderada.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDysEnKVZErk",
        "outputId": "fa381a4f-8c1e-4731-c13e-9d73754207ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similidaridade entre cachorro e gato: -0.027537165209650993\n",
            "Similidaridade entre cachorro e bola: 0.08071544766426086\n",
            "Similidaridade entre céu e lua: 0.16293543577194214\n",
            "Similidaridade entre computador e mesa: 0.037479717284440994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 3 - Glove"
      ],
      "metadata": {
        "id": "pwilK9BrUPp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação da biblioteca\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Indicação do caminho do arquivo\n",
        "glove_path = '/content/drive/MyDrive/[FATEC] PLN/glove.6B.100d.txt'\n",
        "\n",
        "# Acesso ao modelo treinado (instanciação)\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
        "  # glove_path >>> caminho do arquivo com o modelo treinado\n",
        "  # binary >>> arquivo em texto (False)\n",
        "  # no_header >>> ignorar o cabeçalho\n",
        "\n",
        "# Método para acessar a similaridade de duas palavras dentro do modelo treinado\n",
        "similaridade = glove_model.similarity('king', 'queen')\n",
        "print(\"Similaridade entre 'king' e 'queen': \", similaridade)\n",
        "\n",
        "# Método de proximidade para uma determinada palavra \"King\"\n",
        "palavras_proximas = glove_model.most_similar('king')\n",
        "print(\"Palavras próximas a 'king': \", palavras_proximas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4io_b_HAZTHj",
        "outputId": "2d7ce424-dce1-4a4d-cd41-b86b435002cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/[FATEC] PLN/glove.6B.100d.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fffa4533deae>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Acesso ao modelo treinado (instanciação)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# glove_path >>> caminho do arquivo com o modelo treinado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# binary >>> arquivo em texto (False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/[FATEC] PLN/glove.6B.100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 4 - FastText"
      ],
      "metadata": {
        "id": "2C-CXg5RXzrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação das funcionalidades necessárias para o uso do modelo FastText\n",
        "from gensim.models import fasttext\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Caminho para o arquivo FastText pré-treinado (modelo com 300 dimensões para o português)\n",
        "fasttext_path = '/content/drive/MyDrive/[FATEC] PLN/cc.pt.300.vec'\n",
        "\n",
        "# Carregamento do modelo FastText em formato compatível com o Word2Vec (ainda que o modelo seja FastText)\n",
        "# O arquivo está em formato texto (não binário), por isso 'binary=False' e 'no_header=True' porque o arquivo não contém cabeçalho\n",
        "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_path, binary=False, no_header=True)\n",
        "\n",
        "# Cálculo da similaridade entre as palavras 'gato' e 'gatinhos'\n",
        "# A similaridade vai retornar um valor entre -1 e 1, onde 1 significa palavras muito semelhantes\n",
        "similaridade = fasttext_model.similarity('gato', 'gatinhos')\n",
        "print(f\"Similaridade entre 'gato' e 'gatinhos': {similaridade:.4f}\")\n",
        "# Espera-se que a similaridade seja alta, já que 'gato' e 'gatinhos' são semanticamente muito próximos (diferem apenas em plural)\n",
        "\n",
        "# Cálculo das palavras mais próximas de 'gato'\n",
        "# A função 'most_similar' retorna as palavras que têm vetores próximos a 'gato' no espaço vetorial\n",
        "palavras_proximas = fasttext_model.most_similar('gato')\n",
        "print(\"Palavras mais próximas de 'gato': \")\n",
        "for palavra, score in palavras_proximas:\n",
        "    # Exibe as palavras mais próximas e seus respectivos scores (semelhança)\n",
        "    print(f\"{palavra}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "f1FxYTh9ZsJ6",
        "outputId": "7329a5f0-8472-4a61-b485-44c012e6c82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/[FATEC] PLN/cc.pt.300.vec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9ac3a946c8a9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Carregamento do modelo FastText em formato compatível com o Word2Vec (ainda que o modelo seja FastText)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# O arquivo está em formato texto (não binário), por isso 'binary=False' e 'no_header=True' porque o arquivo não contém cabeçalho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfasttext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasttext_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Cálculo da similaridade entre as palavras 'gato' e 'gatinhos'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/[FATEC] PLN/cc.pt.300.vec'"
          ]
        }
      ]
    }
  ]
}
